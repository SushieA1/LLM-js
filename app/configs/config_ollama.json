{
    "model": "llama2",
    "messages": [
      {
        "role": "system",
        "content": ""
      }
    ],
    "options": {
        "top_p": 0.95,
        "temperature": 0.7
    },
    "stream": false
}
